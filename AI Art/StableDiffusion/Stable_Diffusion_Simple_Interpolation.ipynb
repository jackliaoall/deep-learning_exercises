{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU52ZvES6-1T"
      },
      "source": [
        "# Stable Diffusion Interpolation V2.2 by [@ygantigravity](https://twitter.com/ygantigravity) and [@pharmapsychotic](https://twitter.com/pharmapsychotic)\n",
        "\n",
        "A simple implementation of generating N interpolated images (i.e. the in-between images) between multiple text prompts + multiple seeds, built on top of [pharmapsychotic's Stable Diffusion notebook](https://colab.research.google.com/github/pharmapsychotic/ai-notebooks/blob/main/pharmapsychotic_Stable_Diffusion.ipynb#scrollTo=DeqQ7pt1zdI7).\n",
        "\n",
        "\n",
        "Change Log\n",
        "- 08232022 v1 support interpolating btw two prompts with a fixed seed\n",
        "- 08252022 v2 support interpolating using a list of seeds and prompts\n",
        "- 08282022 v2.1 added support for init_image + generate video\n",
        "- 09132022 v2.2 fixed blurring issue for multi-seed\n",
        "\n",
        "Don't forget to checkout Deforum's SD notebook! Interpolation is implemented there as well :)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uc5OwvKdjRJF"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusion\"\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XA1NNcxM724U"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mTjVEfsNlDly"
      },
      "outputs": [],
      "source": [
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcHsbr3hblrk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch.cuda.amp import autocast\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'full' # 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "      \n",
        "def load_img(path, w, h):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize((w, h), Image.LANCZOS)\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "target_c = None\n",
        "target_z = None\n",
        "\n",
        "def slerp(val, low, high):\n",
        "    low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos((low_norm*high_norm).sum(1))\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.0-val)*omega)/so).unsqueeze(1)*low + (torch.sin(val*omega)/so).unsqueeze(1) * high\n",
        "    return res\n",
        "\n",
        "def generate(opt):\n",
        "    global target_c, target_z\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    #make a placeholder for returning c, z\n",
        "    c = None\n",
        "    z = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, opt.W, opt.H).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    # if opt.fixed_code and init_latent == None:\n",
        "        # start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "    if opt.fixed_code:\n",
        "        if target_z != None:\n",
        "          start_code = target_z\n",
        "        else: \n",
        "          start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "          z = start_code\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    images = []\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        #force generation using an interpolated target c\n",
        "                        if target_c != None:\n",
        "                          c = target_c\n",
        "                        \n",
        "                        if init_latent != None:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device), noise=start_code)\n",
        "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "                            \n",
        "                        else:\n",
        "\n",
        "                            if opt.sampler == 'klms':\n",
        "                                print(\"Using KLMS sampling\")\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                                # x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                if target_z != None:\n",
        "                                  x = target_z\n",
        "                                else: \n",
        "                                  x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                  z = x\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                                samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                            else:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=opt.n_samples,\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=opt.ddim_eta,\n",
        "                                                                x_T=start_code)\n",
        "                      \n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        for x_sample in x_samples:\n",
        "                            \n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_{sample_idx:04}.png\")\n",
        "                            print(f\"Saving to {filepath}\")\n",
        "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                            sample_idx += 1\n",
        "    return images, c, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzmVAdZ1-5tE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Image creation\n",
        "#@markdown Note: these settings will be used for all text prompts\n",
        "\n",
        "#@markdown `batch_name`: name for subfolder and filenames<br>\n",
        "#@markdown `width_height`: image dimensions<br>\n",
        "#@markdown `guidance_scale`: strength of text prompt<br>\n",
        "#@markdown `steps`: number of diffusion steps<br>\n",
        "#@markdown `num_batch_images`: how many images you want to generate in this batch. should be 1<br>\n",
        "#@markdown `sampler`: KLMS is recommended<br>\n",
        "#@markdown `ddim_eta`: scale of variance from 0.0 to 1.0<br>\n",
        "#@markdown `seed`: will be overwritten below<br>\n",
        "#@markdown `init_image_or_folder`: url or path to an image, or path to a folder to pick random images from<br>\n",
        "#@markdown `init_strength`: from 0.0 to 1.0 how much the init image is used<br>\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown Batch settings\n",
        "batch_name = \"20220828_Interpolate_Test\" #@param {type:\"string\"}\n",
        "width_height = [512, 512] #@param{type: 'raw'}\n",
        "guidance_scale = 7 #@param {type:\"number\"}\n",
        "steps = 20 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "num_batch_images = 1 #@param {type:\"integer\"}\n",
        "sampler = 'klms' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 #@param {type:\"number\"}\n",
        "seed = 1234 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown Init image\n",
        "\n",
        "\n",
        "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
        "init_strength = 0 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "\n",
        "opt.init_img = init_image_or_folder\n",
        "opt.ddim_steps = steps\n",
        "opt.n_iter = 1\n",
        "opt.n_samples = samples_per_batch\n",
        "opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "opt.sampler = sampler\n",
        "opt.scale = guidance_scale\n",
        "opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "opt.W = width_height[0]\n",
        "opt.H = width_height[1]\n",
        "\n",
        "if opt.strength >= 1 or init_image_or_folder == None:\n",
        "    opt.init_img = \"\"\n",
        "\n",
        "if opt.init_img != None and opt.init_img != '':\n",
        "    opt.sampler = 'ddim'\n",
        "\n",
        "if opt.sampler != 'ddim':\n",
        "    opt.ddim_eta = 0.0\n",
        "\n",
        "print('Settings ready. Now go ahead and generate the images seeds and prompts.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpolate N Prompts"
      ],
      "metadata": {
        "id": "7GJBMGq5EojH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seeds_and_prompts = {\n",
        "    0: {\"seed\": 1234, \"prompt\":\"a beautiful apple, watercolor painting by Hayao Miyazaki, trending on artstation\"},\n",
        "    1: {\"seed\": 3456, \"prompt\":\"a beautiful banana, watercolor painting by Hayao Miyazaki, trending on artstation\"},\n",
        "    2: {\"seed\": 5678, \"prompt\":\"a beautiful coconut, watercolor painting by Hayao Miyazaki, trending on artstation\"},\n",
        "}"
      ],
      "metadata": {
        "id": "J_eWZqmj5NCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1: Generate 1 Image For Each Prompt\n",
        "prompts_c_s = {}\n",
        "prompts_z_s = {}\n",
        "target_c = None # clear target_c\n",
        "target_z = None # clear target_z\n",
        "\n",
        "\n",
        "# save settings\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'num_batch_images': num_batch_images,\n",
        "    'prompt': seeds_and_prompts,\n",
        "    'sampler': sampler,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seeds_and_prompts': seeds_and_prompts,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "sample_idx = 0\n",
        "\n",
        "for i in range(num_batch_images):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    for j, seed_and_prompt in seeds_and_prompts.items():\n",
        "      opt.seed = seed_and_prompt['seed']\n",
        "      opt.prompt = seed_and_prompt['prompt']\n",
        "      print(f\"Used seed: {opt.seed}\")\n",
        "      print(f\"Saved to: {opt.outdir}\")\n",
        "\n",
        "      # opt.init_img = None #clear init\n",
        "      # if seed_and_prompt['init_image'] != None and seed_and_prompt['init_image'] != \"\":\n",
        "      #   opt.init_img = seed_and_prompt['init_image']\n",
        "      images, prompt_c, prompt_z = generate(opt)\n",
        "\n",
        "      #save all the Cs and Zs\n",
        "      prompts_c_s[j] = prompt_c\n",
        "      prompts_z_s[j] = prompt_z\n",
        "\n",
        "      for image in images:\n",
        "          display(image)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h5ZeebtSEygv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [Optional] Step 1.5: Regenerate 1 Image For Prompt X \n",
        "\n",
        "#@markdown 1. Use this to regenerate using a new prompt or seed if you're not satisfied. id should match the id inside \"seeds_and_prompts\" for that prompt.<br>\n",
        "#@markdown 2. To revert, re-generate again with the original seed and prompt.\n",
        "id = 2 #@param {type:\"integer\"}\n",
        "new_seed = 1234 #@param {type:\"integer\"}\n",
        "new_prompt = \"a beautiful coconut, watercolor painting by Hayao Miyazaki, trending on artstation\" #@param {type:\"string\"}\n",
        "\n",
        "opt.seed = new_seed\n",
        "opt.prompt = new_prompt\n",
        "target_c = None # clear target_c\n",
        "target_z = None # clear target_z\n",
        "\n",
        "# save settings\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'num_batch_images': num_batch_images,\n",
        "    'prompt': opt.prompt,\n",
        "    'sampler': sampler,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seed': opt.seed,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "sample_idx = 0\n",
        "\n",
        "for i in range(num_batch_images):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    images, prompt_c, prompt_z = generate(opt)\n",
        "\n",
        "    #replace prompt and c\n",
        "    seeds_and_prompts[id]['prompt'] = new_prompt\n",
        "    prompts_c_s[id] = prompt_c\n",
        "    prompts_z_s[id] = prompt_z\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Used seed: {opt.seed}\")\n",
        "    print(f\"Saved to: {opt.outdir}\")\n",
        "    for image in images:\n",
        "        display(image)\n",
        "\n",
        "    #opt.seed += 1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9FxLpbiBE3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interpolate All Prompts!\n",
        "target_c = None # clear target_c\n",
        "target_z = None # clear target_z\n",
        "\n",
        "# save settings\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'num_batch_images': num_batch_images,\n",
        "    'prompt': opt.prompt,\n",
        "    'sampler': sampler,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seeds_and_prompts': seeds_and_prompts,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "sample_idx = 0\n",
        "\n",
        "interpolate_frames = 24 #@param {type:\"integer\"}\n",
        "for i in range(len(prompts_c_s)-1):\n",
        "  for j in range(interpolate_frames+1):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # interpolating c and z\n",
        "    prompt1_c = prompts_c_s[i]\n",
        "    prompt2_c = prompts_c_s[i+1]  \n",
        "    target_c = prompt1_c.add(prompt2_c.sub(prompt1_c).mul(j * 1/interpolate_frames))\n",
        "\n",
        "    prompt1_z = prompts_z_s[i]\n",
        "    prompt2_z = prompts_z_s[i+1]\n",
        "    target_z = slerp(j * 1/interpolate_frames, prompt1_z, prompt2_z)\n",
        "\n",
        "    images, _, _ = generate(opt)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    # print(f\"Used seed: {opt.seed}\")\n",
        "    print(f\"Saved to: {opt.outdir}\")\n",
        "    for image in images:\n",
        "        display(image)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mMUktc1YFEuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make a Video\n",
        "fps = 12#@param {type:\"number\"}\n",
        "\n",
        "filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_%04d.png\")\n",
        "\n",
        "import subprocess\n",
        "from base64 import b64encode\n",
        "\n",
        "image_path = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_%04d.png\")\n",
        "mp4_path = os.path.join(opt.outdir, f\"{batch_name}({batch_idx}).mp4\")\n",
        "\n",
        "print(f\"{image_path} -> {mp4_path}\")\n",
        "\n",
        "# make video\n",
        "cmd = [\n",
        "    'ffmpeg',\n",
        "    '-y',\n",
        "    '-vcodec', 'png',\n",
        "    '-r', str(fps),\n",
        "    '-start_number', str(0),\n",
        "    '-i', image_path,\n",
        "    '-c:v', 'libx264',\n",
        "    '-vf',\n",
        "    f'fps={fps}',\n",
        "    '-pix_fmt', 'yuv420p',\n",
        "    '-crf', '17',\n",
        "    '-preset', 'veryfast',\n",
        "    mp4_path\n",
        "]\n",
        "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    print(stderr)\n",
        "    raise RuntimeError(stderr)\n",
        "\n",
        "mp4 = open(mp4_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display(HTML(f'<video controls loop><source src=\"{data_url}\" type=\"video/mp4\"></video>') )"
      ],
      "metadata": {
        "id": "T8q8G3g5Hm8p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}